{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network from Scratch\n",
    "\n",
    "Our objective is to build a neural network for the classification of the MNIST dataset.\n",
    "*  [MNIST](http://yann.lecun.com/exdb/mnist/) dataset contains images of handwritten digits of size 28x28. We will be importing this dataset using keras.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://user-images.githubusercontent.com/81357954/166119893-4ca347b8-b1a4-40b8-9e0a-2e92b5f164ae.png\">\n",
    "</center>\n",
    "\n",
    " This neural network will comprise of: an output layer with 10 nodes, a hidden layer of 256 nodes, and an input layer with 784 nodes corresponding to the image pixels. The specific structure of the neural network is outlined below, where $X$ represents the input, $A^{[0]}$ denotes the first layer, $Z^{[1]}$ signifies the unactivated layer 1, $A^{[1]}$ stands for the activated layer 1, and so forth. The weights and biases are represented by $W$ and $b$ respectively:\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "$A^{[0]}=X$\n",
    "\n",
    "$Z^{[1]}=W^{[1]}A^{[0]}+b^{[1]}$\n",
    "\n",
    "$A^{[1]}=\\text{ReLU}(Z^{[1]})$\n",
    "\n",
    "$Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}$\n",
    "\n",
    "$A^{[2]}=\\text{softmax}(Z^{[2]})$\n",
    "\n",
    "$Loss=\\text{cross-entropy-loss}(A^{[2]})$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "\n",
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of the activation function(ReLU) and softmax function\n",
    "\n",
    "* Use ReLU for the hidden layer and Softmax function for the output layer for classification into 10 different neurons, each corresponding to a particular number between $1$ and $10$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(Z):\n",
    "    pass\n",
    "\n",
    "def softmax(Z):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the NN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        # initialized basic stats of NN\n",
    "        self.input_size=input_size\n",
    "        self.hidden_size=hidden_size\n",
    "        self.output_size=output_size\n",
    "        self.learning_rate=learning_rate\n",
    "\n",
    "        #initialized weights and biases\n",
    "        self.W1=np.random.randn(hidden_size, input_size)*0.01\n",
    "        self.B1= np.zeros((1,hidden_size))\n",
    "        self.W2=np.random.randn(output_size, hidden_size)*0.01\n",
    "        self.B2=np.zeros((1,output_size))\n",
    "\n",
    "        #initialized activations and gradients\n",
    "        self.AO=None\n",
    "        self.Z1=None\n",
    "        self.A1=None\n",
    "        self.Z2=None\n",
    "        self.A2=None\n",
    "        self.dZ2=None\n",
    "        self.dW2=None\n",
    "        self.dB2=None\n",
    "        self.dZ1=None\n",
    "        self.dW1=None\n",
    "        self.dB1=None\n",
    "\n",
    "    # do the forward pass and evaluate the values of A0, Z1, A1, Z2, A2\n",
    "    def forward_propagation(self, X):\n",
    "        pass\n",
    "        \n",
    "    \n",
    "    # convert the input y, into a one hot encoded array.\n",
    "    '''\n",
    "    one hot encoding is:\n",
    "    you have an array with values [2, 5, 6] and you know the max value can be 8, then one hot encoded array will be:\n",
    "    [[0,0,1,0,0,0,0,0,0], [0,0,0,0,0,1,0,0,0], [0,0,0,0,0,0,1,0,0]]\n",
    "    Note that the index 2, 5, 6 have values 1 and all others have values 0\n",
    "    '''\n",
    "    def one_hot(self, y):\n",
    "        pass\n",
    "\n",
    "    # calculate the derivative of the cost function with respect to W2, B2, W1, B1 in dW2, dB2, dW1, dB1 respectively\n",
    "    def backward_propagation(self, X, y):\n",
    "        pass\n",
    "    \n",
    "    # update the parameters W1, W2, B1, B2\n",
    "    def update_params(self):\n",
    "        pass\n",
    "    \n",
    "    # get the predictions for the dataset\n",
    "    def get_predictions(self,X):\n",
    "        pass\n",
    "\n",
    "    # get the accuracy of the model on the dataset\n",
    "    # (after the gradient descent has been run separately)\n",
    "    def get_accuracy(self, X, y):\n",
    "        pass\n",
    "    \n",
    "    # run gradient descent on the model to get the values of the parameters\n",
    "    def gradient_descent(self, X, y, iters=1000):\n",
    "        pass\n",
    "    \n",
    "    # evaluate cost using cross-entropy-loss formula.\n",
    "    def cross_entropy_loss(self, X, y):\n",
    "        pass\n",
    "\n",
    "    # Function for random outputs to get an intuitive feel\n",
    "    def show_predictions(self, X, y, num_samples=10):\n",
    "        random_indices = np.random.randint(0, X.shape[0], size=num_samples)\n",
    "\n",
    "        for index in random_indices:\n",
    "            sample_image = X[index, :].reshape((28, 28))\n",
    "            plt.imshow(sample_image, cmap='gray')\n",
    "            plt.title(f\"Actual: {y[index]}, Predicted: {self.get_predictions(X[index])}\")\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "miu = np.mean(X_train, axis=(0, 1), keepdims=True)\n",
    "stds = np.std(X_train, axis=(0, 1), keepdims=True)\n",
    "\n",
    "mius = np.mean(X_test, axis=(0, 1), keepdims=True)\n",
    "stdse = np.std(X_test, axis=(0, 1), keepdims=True)\n",
    "\n",
    "X_normal_train = (X_train - miu) / (stds + 1e-7)\n",
    "X_normal_test = (X_test - mius) / (stdse + 1e-7)\n",
    "\n",
    "X_normal_train = X_normal_train.reshape((60000, -1))\n",
    "X_normal_test = X_normal_test.reshape((10000, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "* Train the model on X_normal_train and Y_train_dataset\n",
    "* Print the accuracy on X_normal_test and Y_test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the model as an instance of the NN class and train it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the show_predictions function to print the actual output and see your model work :)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
